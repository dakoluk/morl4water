diff --git a/.DS_Store b/.DS_Store
index b74fc3c..bb8f5af 100644
Binary files a/.DS_Store and b/.DS_Store differ
diff --git a/configs/capql.yaml b/configs/capql.yaml
index 347f832..5b44dfb 100644
--- a/configs/capql.yaml
+++ b/configs/capql.yaml
@@ -1,62 +1,29 @@
-method: bayes
+method: random
 metric:
   goal: maximize
   name: avg_hypervolume
 parameters:
 
-# The soft update coefficient. Defaults to 0.005.
-  tau:
-    distribution: uniform
-    min: 0.0
-    max: 1.0
-
-#  The discount factor. Defaults to 0.99.   
-  gamma:
-    distribution: uniform
-    min: 0.9
-    max: 1.0
 
 #  The learning rate. Defaults to 3e-4.    
   learning_rate:
-    distribution: uniform
-    min: 0.0001
-    max: 0.01
+    values: [0.0002, 0.0003, 0.0004]
 
 # The size of the replay buffer. Defaults to int(1e6).
   buffer_size:
-    distribution: int_uniform
-    min: 1000
-    max: 2000000
+    values: [1000000, 1500000]
 
 # The network architecture for the policy and Q-networks.
   net_arch:
-    value: [256, 256]
+    value: [512, 512] #, [256, 512]]
 
-# The batch size for training. Defaults to 256.
+# The batch size for training. Defaults to 32.
   batch_size:
-    values: [126, 256, 512]
-
-# The number of Q-networks to use. Defaults to 2.
-  num_q_nets:
-    values: [2, 4, 6]
-
-# The entropy regularization coefficient. Defaults to 0.2.
-  alpha:
-    distribution: uniform
-    min: 0
-    max: 1
-
+    values: 512
 
 # The number of steps to take before starting to train. Defaults to 100.
   learning_starts:
-    distribution: int_uniform
-    min: 1
-    max: 1000
-
-# The number of gradient steps to take per update. Defaults to 1.
-  gradient_updates:
-    distribution: int_uniform
-    min: 1
-    max: 10
-
+    values: [10, 100]
 
+  gamma:
+    value: 1.0
diff --git a/configs/gpi.yaml b/configs/gpi.yaml
index b0382c3..24b34e9 100644
--- a/configs/gpi.yaml
+++ b/configs/gpi.yaml
@@ -1,132 +1,37 @@
-method: bayes
+method: random
 metric:
   goal: maximize
-  name: avg_hypervolume
+  name: hypervolume
 parameters:
 
-# The soft update coefficient. Defaults to 0.005.
-  tau:
-    distribution: uniform
-    min: 0.0
-    max: 1.0
-
-#  The discount factor. Defaults to 0.99.   
-  gamma:
-    distribution: uniform
-    min: 0.9
-    max: 1.0
-
 #  The learning rate. Defaults to 3e-4.    
   learning_rate:
-    distribution: uniform
-    min: 0.0001
-    max: 0.01
+    values: [0.0002, 0.0003, 0.0004]
 
 # The size of the replay buffer. Defaults to int(1e6).
   buffer_size:
-    distribution: int_uniform
-    min: 1000
-    max: 2000000
+    values: [1000000, 1500000]
 
-# The network architecture for the policy and Q-networks.
-  net_arch:
-    value: [256, 256]
-
-# The network architecture for the dynamics model.
-  dynamics_net_arch:
-    value: [200, 200, 200, 200]
-
-# The batch size for training. Defaults to 256.
-  batch_size:
-    value: 256
-
-# The number of Q-networks to use. Defaults to 2.
-  num_q_nets:
-    value: 2
-
-# The number of gradient steps to take before updating the policy. Defaults to 2.
-  delay_policy_update:
-    distribution: int_uniform
-    min: 2
-    max: 5
 
 # The number of steps to take before starting to train. Defaults to 100.
   learning_starts:
-    distribution: int_uniform
-    min: 1
-    max: 1000
-
-# The number of gradient steps to take per update. Defaults to 1.
-  gradient_updates:
-    distribution: int_uniform
-    min: 1
-    max: 10
-
-# The noise to add to the policy. Defaults to 0.2.
-  policy_noise:
-    value: 0.2
+    values: [10, 100]
 
-# The noise clipping value. Defaults to 0.5.
-  noise_clip:
-    value: 0.5
-
-#  The minimum priority to use for prioritized experience replay. Defaults to 0.1.
-  min_priority:
-    value: 0.1
-
-# The alpha value for prioritized experience replay. Defaults to 0.6.
-  alpha: 
-    value: 0.6
-
-# The frequency with which to train the dynamics model. Defaults to 1000.
-  dynamics_train_freq:
-    distribution: int_uniform
-    min: 100
-    max: 1000
-
-# The rollout length for the dynamics model. Defaults to 1.
-  dynamics_rollout_len:
-    distribution: int_uniform
-    min: 1
-    max: 10
-
-#  The number of steps to take before starting to train the dynamics model. Defaults to 5000.
-  dynamics_rollout_starts:
-    distribution: int_uniform
-    min: 1000
-    max: 5000
-
-# The frequency with which to rollout the dynamics model. Defaults to 250.
-  dynamics_rollout_freq:
-    distribution: int_uniform
-    min: 100
-    max: 300
-
-#  The batch size for the dynamics model rollout. Defaults to 10000.
-  dynamics_rollout_batch_size:
-    distribution: int_uniform
-    min: 10000
-    max: 50000
+  gamma:
+    value: 1.0
 
-# The size of the dynamics model replay buffer. Defaults to 400000.
-  dynamics_buffer_size:
-    distribution: int_uniform
-    min: 100000
-    max: 400000
 
-# The minimum uncertainty to use for the dynamics model. Defaults to 1.0.
-  dynamics_min_uncertainty:
-    distribution: uniform
-    min: 1.0
-    max: 2.0
+# The network architecture for the policy and Q-networks.
+  net_arch:
+    value: [512, 512] #, [256, 512]]
 
-# The ratio of real data to use for the dynamics model. Defaults to 0.1.
-  dynamics_real_ratio:
-    distribution: uniform
-    min: 0.1
-    max: 1.0
+# The network architecture for the dynamics model.
+  dynamics_net_arch:
+    value: [200, 200, 200, 200]
 
+# The batch size for training. Defaults to 256. maybe change to smaller
+  batch_size:
+    value: 512
 
 
 
- 
diff --git a/configs/pcn.yaml b/configs/pcn.yaml
index 4bf4f2b..c299a7f 100644
--- a/configs/pcn.yaml
+++ b/configs/pcn.yaml
@@ -1,34 +1,21 @@
-method: bayes
+method: random
 metric:
   goal: maximize
   name: avg_hypervolume
 parameters:
 
 
-  #  The discount factor. Defaults to 0.99.   
-  gamma:
-    distribution: uniform
-    min: 0.9
-    max: 1.0
-
 #  The learning rate. Defaults to 3e-4.    
   learning_rate:
-    distribution: uniform
-    min: 0.0001
-    max: 0.01
+    values: [0.0002, 0.0003, 0.0004]
 
 # The batch size for training. Defaults to 32.
   batch_size:
-    values: [32, 64, 128, 256]
+    values: 512
 
 # Hidden dimension. Defaults to 64.
   hidden_dim:
-    values: [32, 64]
-
-# Standard deviation of the noise to add to the action in the continuous action case. Defaults to 0.1.
-  noise:
-    distribution: uniform
-    min: 0.01
-    max: 0.2
-
+    values: [64, 128]
 
+  gamma:
+    value: 1.0
diff --git a/core/envs/water_management_system.py b/core/envs/water_management_system.py
index 6a2e400..7e6bdc6 100644
--- a/core/envs/water_management_system.py
+++ b/core/envs/water_management_system.py
@@ -48,10 +48,6 @@ class WaterManagementSystem(gym.Env):
 
         self.observation: np.array = self._determine_observation()
 
-        
-        
-        
-
         for water_system in self.water_systems:
             water_system.current_date = self.current_date
             water_system.timestep_size = self.timestep_size
diff --git a/launch_experiment.py b/launch_experiment.py
index ff7a3c8..5433a27 100644
--- a/launch_experiment.py
+++ b/launch_experiment.py
@@ -3,7 +3,8 @@
 Many parameters can be given in the command line, see the help for more infos.
 
 Examples:
-    python benchmark/launch_experiment.py --algo pcn --env-id deep-sea-treasure-v0 --num-timesteps 1000000 --gamma 0.99 --ref-point 0 -25 --auto-tag True --wandb-entity openrlbenchmark --seed 0 --init-hyperparams "scaling_factor:np.array([1, 1, 1])"
+    python benchmark/launch_experiment.py --algo pcn --env-id deep-sea-treasure-v0 --num-timesteps 1000000 --gamma 0.99 --ref-point 0 -25 --auto-tag True --wandb-entity openrlbenchmark 
+    --seed 0 --init-hyperparams "scaling_factor:np.array([1, 1, 1])"
 """
 
 import argparse
diff --git a/launch_sweep.py b/launch_sweep.py
deleted file mode 100644
index de48d78..0000000
--- a/launch_sweep.py
+++ /dev/null
@@ -1,188 +0,0 @@
-import argparse
-import os
-from concurrent.futures import ProcessPoolExecutor
-from dataclasses import dataclass
-
-import mo_gymnasium as mo_gym
-import numpy as np
-import wandb
-import yaml
-from mo_gymnasium.utils import MORecordEpisodeStatistics
-
-from morl_baselines.common.evaluation import seed_everything
-from morl_baselines.common.experiments import (
-    ALGOS,
-    ENVS_WITH_KNOWN_PARETO_FRONT,
-    StoreDict,
-)
-from morl_baselines.common.utils import reset_wandb_env
-
-
-@dataclass
-class WorkerInitData:
-    sweep_id: str
-    seed: int
-    config: dict
-    worker_num: int
-
-
-@dataclass
-class WorkerDoneData:
-    hypervolume: float
-
-
-def parse_args():
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--algo", type=str, help="Name of the algorithm to run", choices=ALGOS.keys(), required=True)
-    parser.add_argument("--env-id", type=str, help="MO-Gymnasium id of the environment to run", required=True)
-    parser.add_argument(
-        "--ref-point", type=float, nargs="+", help="Reference point to use for the hypervolume calculation", required=True
-    )
-
-    parser.add_argument("--wandb-entity", type=str, help="Wandb entity to use for the sweep", required=False)
-    parser.add_argument("--project-name", type=str, help="Project name to use for the sweep", default="MORL-Baselines")
-
-    parser.add_argument("--sweep-count", type=int, help="Number of trials to do in the sweep worker", default=10)
-    parser.add_argument("--num-seeds", type=int, help="Number of seeds to use for the sweep", default=3)
-
-    parser.add_argument(
-        "--seed", type=int, help="Random seed to start from, seeds will be in [seed, seed+num-seeds)", default=10
-    )
-
-    parser.add_argument(
-        "--train-hyperparams",
-        type=str,
-        nargs="+",
-        action=StoreDict,
-        help="Override hyperparameters to use for the train method algorithm. Example: --train-hyperparams num_eval_weights_for_front:10 timesteps_per_iter:10000",
-        default={},
-    )
-
-    parser.add_argument(
-        "--config-name",
-        type=str,
-        help="Name of the config to use for the sweep, defaults to using the same name as the algorithm.",
-    )
-
-    args = parser.parse_args()
-
-    if not args.config_name:
-        args.config_name = f"{args.algo}.yaml"
-    elif not args.config_name.endswith(".yaml"):
-        args.config_name += ".yaml"
-
-    return args
-
-
-def train(worker_data: WorkerInitData) -> WorkerDoneData:
-    # Reset the wandb environment variables
-    reset_wandb_env()
-
-    seed = worker_data.seed
-    group = worker_data.sweep_id
-    config = worker_data.config
-    worker_num = worker_data.worker_num
-
-    # Set the seed
-    seed_everything(seed)
-
-    if args.algo == "pgmorl":
-        # PGMORL creates its own environments because it requires wrappers
-        print(f"Worker {worker_num}: Seed {seed}. Instantiating {args.algo} on {args.env_id}")
-        eval_env = mo_gym.make(args.env_id)
-        algo = ALGOS[args.algo](
-            env_id=args.env_id,
-            origin=np.array(args.ref_point),
-            wandb_entity=args.wandb_entity,
-            **config,
-            seed=seed,
-            group=group,
-        )
-
-        # Launch the agent training
-        print(f"Worker {worker_num}: Seed {seed}. Training agent...")
-        algo.train(
-            eval_env=eval_env,
-            ref_point=np.array(args.ref_point),
-            known_pareto_front=None,
-            **args.train_hyperparams,
-        )
-
-    else:
-        print(f"Worker {worker_num}: Seed {seed}. Instantiating {args.algo} on {args.env_id}")
-        env = MORecordEpisodeStatistics(mo_gym.make(args.env_id), gamma=config["gamma"])
-        eval_env = mo_gym.make(args.env_id)
-
-        algo = ALGOS[args.algo](env=env, wandb_entity=args.wandb_entity, **config, seed=seed, group=group)
-
-        if args.env_id in ENVS_WITH_KNOWN_PARETO_FRONT:
-            known_pareto_front = env.unwrapped.pareto_front(gamma=config["gamma"])
-        else:
-            known_pareto_front = None
-
-        # Launch the agent training
-        print(f"Worker {worker_num}: Seed {seed}. Training agent...")
-        algo.train(
-            eval_env=eval_env,
-            ref_point=np.array(args.ref_point),
-            known_pareto_front=known_pareto_front,
-            **args.train_hyperparams,
-        )
-
-    # Get the hypervolume from the wandb run
-    hypervolume = wandb.run.summary["eval/hypervolume"]
-    print(f"Worker {worker_num}: Seed {seed}. Hypervolume: {hypervolume}")
-
-    return WorkerDoneData(hypervolume=hypervolume)
-
-
-def main():
-    # Get the sweep id
-    sweep_run = wandb.init()
-
-    # Spin up workers before calling wandb.init()
-    # Workers will be blocked on a queue waiting to start
-    with ProcessPoolExecutor(max_workers=args.num_seeds) as executor:
-        futures = []
-        for num in range(args.num_seeds):
-            # print("Spinning up worker {}".format(num))
-            seed = seeds[num]
-            futures.append(
-                executor.submit(
-                    train, WorkerInitData(sweep_id=sweep_id, seed=seed, config=dict(sweep_run.config), worker_num=num)
-                )
-            )
-
-        # Get results from workers
-        results = [future.result() for future in futures]
-
-    # Get the hypervolume from the results
-    hypervolume_metrics = [result.hypervolume for result in results]
-    print(f"Hypervolumes of the sweep {sweep_id}: {hypervolume_metrics}")
-
-    # Compute the average hypervolume
-    average_hypervolume = sum(hypervolume_metrics) / len(hypervolume_metrics)
-    print(f"Average hypervolume of the sweep {sweep_id}: {average_hypervolume}")
-
-    # Log the average hypervolume to the sweep run
-    sweep_run.log(dict(avg_hypervolume=average_hypervolume))
-    wandb.finish()
-
-
-args = parse_args()
-
-# Create an array of seeds to use for the sweep
-seeds = [args.seed + i for i in range(args.num_seeds)]
-
-# Load the sweep config
-config_file = os.path.join(os.path.dirname(__file__), "configs", args.config_name)
-
-# Set up the default hyperparameters
-with open(config_file) as file:
-    sweep_config = yaml.load(file, Loader=yaml.FullLoader)
-
-# Set up the sweep
-sweep_id = wandb.sweep(sweep=sweep_config, entity=args.wandb_entity, project=args.project_name)
-
-# Run the sweep agent
-wandb.agent(sweep_id, function=main, count=args.sweep_count)
diff --git a/morl_baselines/multi_policy/gpi_pd/gpi_pd_continuous_action.py b/morl_baselines/multi_policy/gpi_pd/gpi_pd_continuous_action.py
index a9e31fb..baf9d96 100644
--- a/morl_baselines/multi_policy/gpi_pd/gpi_pd_continuous_action.py
+++ b/morl_baselines/multi_policy/gpi_pd/gpi_pd_continuous_action.py
@@ -117,6 +117,8 @@ class GPIPDContinuousAction(MOAgent, MOPolicy):
         log: bool = True,
         seed: Optional[int] = None,
         device: Union[th.device, str] = "auto",
+        group: Optional[str] = None,
+
     ):
         """GPI-PD algorithm with continuous actions.
 
@@ -240,7 +242,7 @@ class GPIPDContinuousAction(MOAgent, MOPolicy):
 
         self.log = log
         if self.log:
-            self.setup_wandb(project_name, experiment_name, wandb_entity)
+            self.setup_wandb(project_name, experiment_name, wandb_entity, group)
 
     def get_config(self):
         """Get the configuration of the agent."""
diff --git a/run_experiment.py b/run_experiment.py
index efbdecc..c3aabc8 100644
--- a/run_experiment.py
+++ b/run_experiment.py
@@ -53,17 +53,18 @@ class TrackProgress:
         for i, solution in enumerate(algorithm.archive):
             temp[i] = np.array(solution.objectives)
             objectives.append(solution.objectives)
-        hv = hypervolume(ref_point=np.array(self.ref_point), points=objectives)
-        sp = sparsity(objectives)
-        cd = cardinality(objectives)
-
-        # Log metrics to wandb
-        wandb.log({
-            
-            "Hypervolume": hv,
-            "Sparsity": sp,
-            "Cardinality": cd
-        }, step=algorithm.nfe)
+        if algorithm.nfe%1000==0:
+            hv = hypervolume(ref_point=np.array(self.ref_point), points=objectives)
+            sp = sparsity(objectives)
+            cd = cardinality(objectives)
+
+            # Log metrics to wandb
+            wandb.log({
+                
+                "Hypervolume": hv,
+                "Sparsity": sp,
+                "Cardinality": cd
+            }, step=algorithm.nfe)
         
         self.objectives[algorithm.nfe] = pd.DataFrame.from_dict(temp, orient="index")
 
diff --git a/wandb/latest-run b/wandb/latest-run
index 6bde093..fad7ec2 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20240918_173611-pich2e5z
\ No newline at end of file
+run-20241003_160314-qjfn9553
\ No newline at end of file
